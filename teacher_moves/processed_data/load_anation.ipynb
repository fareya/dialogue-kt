{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "import os "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22\n",
      "Index(['useraccount_id', 'Success', 'confirmatory feedback',\n",
      "       'negative feedback', 'correcting', 'giving instruction',\n",
      "       'giving explanation', 'providing further references', 'questioning',\n",
      "       'asking for elaboration', 'praising and encouraging',\n",
      "       'managing frustration', 'managing discussions', 'giving answers',\n",
      "       'encouraging peer tutoring', 'guiding peer tutoring',\n",
      "       'acknowledging tutor issue', 'irrelevant statement',\n",
      "       'computational skill', 'linguistic knowledge', 'conceptual knowledge',\n",
      "       'strategic knowledge'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "file = \"/Users/fareyaikram/Workspace/dialogue-kt-main/fareya/raw_data/LEVI_tutoring_dataset_round1.csv\"\n",
    "df = pd.read_csv(file)\n",
    "my_cols = df.columns[4:-1]\n",
    "print(len(my_cols))\n",
    "print(my_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2512\n"
     ]
    }
   ],
   "source": [
    "# find the unique ids in the dataframe\n",
    "print(len(df['id'].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'float' object has no attribute 'split'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m df_no_duplicates \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mdrop_duplicates([\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m----> 2\u001b[0m df_no_duplicates[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprocessed_text\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mdf_no_duplicates\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcontent\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m*document\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m \n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(df_no_duplicates[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprocessed_text\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mhead(\u001b[38;5;241m40\u001b[39m))\n",
      "File \u001b[0;32m~/Workspace/dialogue-kt-main/dk/lib/python3.9/site-packages/pandas/core/frame.py:10374\u001b[0m, in \u001b[0;36mDataFrame.apply\u001b[0;34m(self, func, axis, raw, result_type, args, by_row, engine, engine_kwargs, **kwargs)\u001b[0m\n\u001b[1;32m  10360\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapply\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m frame_apply\n\u001b[1;32m  10362\u001b[0m op \u001b[38;5;241m=\u001b[39m frame_apply(\n\u001b[1;32m  10363\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m  10364\u001b[0m     func\u001b[38;5;241m=\u001b[39mfunc,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m  10372\u001b[0m     kwargs\u001b[38;5;241m=\u001b[39mkwargs,\n\u001b[1;32m  10373\u001b[0m )\n\u001b[0;32m> 10374\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39m__finalize__(\u001b[38;5;28mself\u001b[39m, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mapply\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Workspace/dialogue-kt-main/dk/lib/python3.9/site-packages/pandas/core/apply.py:916\u001b[0m, in \u001b[0;36mFrameApply.apply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    913\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw:\n\u001b[1;32m    914\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_raw(engine\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine, engine_kwargs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine_kwargs)\n\u001b[0;32m--> 916\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Workspace/dialogue-kt-main/dk/lib/python3.9/site-packages/pandas/core/apply.py:1063\u001b[0m, in \u001b[0;36mFrameApply.apply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1061\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mapply_standard\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m   1062\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpython\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m-> 1063\u001b[0m         results, res_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_series_generator\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1064\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1065\u001b[0m         results, res_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_series_numba()\n",
      "File \u001b[0;32m~/Workspace/dialogue-kt-main/dk/lib/python3.9/site-packages/pandas/core/apply.py:1081\u001b[0m, in \u001b[0;36mFrameApply.apply_series_generator\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1078\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m option_context(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmode.chained_assignment\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m   1079\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(series_gen):\n\u001b[1;32m   1080\u001b[0m         \u001b[38;5;66;03m# ignore SettingWithCopy here in case the user mutates\u001b[39;00m\n\u001b[0;32m-> 1081\u001b[0m         results[i] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1082\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(results[i], ABCSeries):\n\u001b[1;32m   1083\u001b[0m             \u001b[38;5;66;03m# If we have a view on v, we need to make a copy because\u001b[39;00m\n\u001b[1;32m   1084\u001b[0m             \u001b[38;5;66;03m#  series_generator will swap out the underlying data\u001b[39;00m\n\u001b[1;32m   1085\u001b[0m             results[i] \u001b[38;5;241m=\u001b[39m results[i]\u001b[38;5;241m.\u001b[39mcopy(deep\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "Cell \u001b[0;32mIn[31], line 2\u001b[0m, in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m      1\u001b[0m df_no_duplicates \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mdrop_duplicates([\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m----> 2\u001b[0m df_no_duplicates[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprocessed_text\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df_no_duplicates\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[43mx\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcontent\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*document\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m0\u001b[39m], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m) \n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(df_no_duplicates[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprocessed_text\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mhead(\u001b[38;5;241m40\u001b[39m))\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'float' object has no attribute 'split'"
     ]
    }
   ],
   "source": [
    "df_no_duplicates = df.drop_duplicates(['content'])\n",
    "df_no_duplicates['processed_text'] = df_no_duplicates.apply(lambda x: x['content'].split(\"*document\")[0], axis=1) \n",
    "print(df_no_duplicates['processed_text'].head(40))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#write a function that underscore between the words in the column names\n",
    "def underscore_column_names(columns):\n",
    "    new_columns = []\n",
    "    for column in columns:\n",
    "        column = column.replace(\" \", \"_\")\n",
    "        new_columns.append(column.lower())\n",
    "    \n",
    "    return str(new_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'id2', 'content', 'Success', 'confirmatory feedback',\n",
       "       'negative feedback', 'correcting', 'giving instruction',\n",
       "       'giving explanation', 'providing further references', 'questioning',\n",
       "       'asking for elaboration', 'praising and encouraging',\n",
       "       'managing frustration', 'managing discussions', 'giving answers',\n",
       "       'encouraging peer tutoring', 'guiding peer tutoring',\n",
       "       'acknowledging tutor issue', 'other', 'irrelevant statement',\n",
       "       'computational skill', 'linguistic knowledge', 'conceptual knowledge',\n",
       "       'strategic knowledge', 'affective control', 'processed_text'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_no_duplicates.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/1j/s7v97cmj2_b4j84vjkcbfkl00000gn/T/ipykernel_3598/460016489.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_no_duplicates['list_of_labels'] = df_no_duplicates.apply(lambda x: underscore_column_names([col for col in my_cols if x[col] == 1]), axis=1)\n"
     ]
    }
   ],
   "source": [
    "df_no_duplicates['list_of_labels'] = df_no_duplicates.apply(lambda x: underscore_column_names([col for col in my_cols if x[col] == 1]), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/1j/s7v97cmj2_b4j84vjkcbfkl00000gn/T/ipykernel_3598/2515613975.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_no_duplicates['processed_text'] = df_no_duplicates['processed_text'].replace(to_replace=r'^[u]\\d+:', value='Student:', regex=True)\n",
      "/var/folders/1j/s7v97cmj2_b4j84vjkcbfkl00000gn/T/ipykernel_3598/2515613975.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_no_duplicates['processed_text'] = df_no_duplicates['processed_text'].replace(to_replace=r'^[p]\\d+:', value='Tutor:', regex=True)\n",
      "/var/folders/1j/s7v97cmj2_b4j84vjkcbfkl00000gn/T/ipykernel_3598/2515613975.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_no_duplicates['processed_text'] = df_no_duplicates['processed_text'].replace(to_replace=r'^[e]\\d+:', value='Teacher:', regex=True)\n"
     ]
    }
   ],
   "source": [
    "# if proccessed_text contains text that starts with 'u' and is followed by a number and a colon, then replace it with 'Student'\n",
    "# if proccessed_text contains text that starts with 'p' and is followed by a number and a colon, then replace it with 'Tutor'\n",
    "\n",
    "df_no_duplicates['processed_text'] = df_no_duplicates['processed_text'].replace(to_replace=r'^[u]\\d+:', value='Student:', regex=True)\n",
    "df_no_duplicates['processed_text'] = df_no_duplicates['processed_text'].replace(to_replace=r'^[p]\\d+:', value='Tutor:', regex=True)\n",
    "df_no_duplicates['processed_text'] = df_no_duplicates['processed_text'].replace(to_replace=r'^[e]\\d+:', value='Teacher:', regex=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/1j/s7v97cmj2_b4j84vjkcbfkl00000gn/T/ipykernel_3598/326357659.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_no_duplicates['is_tutor'] = df_no_duplicates.apply(lambda x: not is_student(x['processed_text']), axis=1)\n"
     ]
    }
   ],
   "source": [
    "def is_student(text):\n",
    "    return text.startswith('Student')\n",
    "df_no_duplicates['is_tutor'] = df_no_duplicates.apply(lambda x: not is_student(x['processed_text']), axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_no_duplicates[['processed_text', 'id', 'id2', 'list_of_labels', 'is_tutor', 'Success']].to_json(\"processed_florida_data_2.jsonl\", orient= 'records',index=False, lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json \n",
    "from collections import defaultdict\n",
    "\n",
    "def read_jsonl(data_path):\n",
    "    with open(data_path) as f:\n",
    "        return [json.loads(line) for line in f]\n",
    "\n",
    "def group_data_by_id(data):\n",
    "    # add all ids to a list\n",
    "    grouped_data = defaultdict(list)\n",
    "    for entry in data:\n",
    "        grouped_data[entry[\"id\"]].append(entry)\n",
    "    \n",
    "    # remove duplicates \n",
    "    # for key in grouped_data.keys():\n",
    "    #     grouped_data[key] = pd.DataFrame(grouped_data[key]).drop_duplicates(subset=['teacher_move', 'student_move']).to_dict('records')\n",
    "    return grouped_data\n",
    "\n",
    "def split_train_val(grouped_data):\n",
    "    train_data, val_data = train_test_split(list(grouped_data.values()), test_size=0.2, random_state=RANDOM_SEED)\n",
    "    return train_data, val_data\n",
    "\n",
    "def get_test_formatted(grouped_data):\n",
    "    test_data = list(grouped_data.values())\n",
    "    return test_data\n",
    "\n",
    "def format_dialogue_anation(data, label_type = \"teacher_move_type\", add_prev_labels = True): \n",
    "    formatted_outputs = []\n",
    "    labels = []\n",
    "    ids = [] \n",
    "    for i, conversation in enumerate(data):\n",
    "        dialogue = []\n",
    "        tutor_labels = []\n",
    "        is_tutor_list = [] \n",
    "        correctness = [] \n",
    "        for j, turn in enumerate(conversation): \n",
    "            dialogue.append(turn['processed_text'])\n",
    "            if turn[\"Success\"]:\n",
    "                correctness.append(\"true\") \n",
    "            else: correctness.append(\"true\") \n",
    "            if turn['is_tutor']: \n",
    "                tutor_labels.append(turn['list_of_labels'])\n",
    "                is_tutor_list.append(True)\n",
    "            else: \n",
    "                tutor_labels.append(None)\n",
    "                is_tutor_list.append(False)\n",
    "\n",
    "        final_dialogue = \"[BEGIN DIALOGUE]\"\n",
    "        for k in range(len(conversation)): \n",
    "            if k>0 and add_prev_labels:\n",
    "                prepend_text = \"\" if not is_tutor_list[k-1] else str(tutor_labels[k-1])\n",
    "            else: \n",
    "                prepend_text = \"\"\n",
    "            final_dialogue = final_dialogue + prepend_text + \"\\n\" + dialogue[k]\n",
    "        \n",
    "            if is_tutor_list[k] and label_type == \"teacher_move_type\":\n",
    "                formatted_outputs.append(final_dialogue + \"\\n[END DIALOGUE]\")\n",
    "                labels.append(tutor_labels[k])\n",
    "                ids.append(str(conversation[k][\"id\"])+str(conversation[k][\"id2\"]))\n",
    "            \n",
    "            if is_tutor_list[k] and label_type == \"final_correctness\":\n",
    "                formatted_outputs.append(final_dialogue + \"\\n[END DIALOGUE]\")\n",
    "                labels.append(correctness[k])\n",
    "                ids.append(str(conversation[k][\"id\"])+str(conversation[k][\"order\"]))\n",
    "\n",
    "            if k < len(conversation) - 1: \n",
    "                if is_tutor_list[k+1] and label_type == \"future_teacher_move_type\":\n",
    "                    formatted_outputs.append(final_dialogue + \"\\n[END DIALOGUE]\")\n",
    "                    labels.append(tutor_labels[k+1])\n",
    "                    ids.append(str(conversation[k][\"id\"])+str(conversation[k][\"order\"]))\n",
    "    final_formatted_outputs = []\n",
    "    final_labels = []\n",
    "    final_ids = [] \n",
    "\n",
    "    for i in range(len(formatted_outputs)): \n",
    "        if not(labels[i] == \"[]\"):\n",
    "            final_formatted_outputs.append(formatted_outputs[i])\n",
    "            final_labels.append(labels[i])\n",
    "            final_ids.append(ids[i])\n",
    "    for i in range(10):\n",
    "        print(final_formatted_outputs[i])\n",
    "        print(final_labels[i])\n",
    "    return final_formatted_outputs, final_labels, final_ids "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = read_jsonl(\"processed_florida_data.jsonl\")\n",
    "grouped_data = group_data_by_id(data)\n",
    "train_data, val_data = split_train_val(grouped_data)\n",
    "# write the train and val data to jsonl files\n",
    "with open(\"anation_train_data.jsonl\", \"w\") as f:\n",
    "    for entry in train_data:\n",
    "        for i in range(len(entry)):\n",
    "            f.write(json.dumps(entry[i]) + \"\\n\")\n",
    "with open(\"anation_val_data.jsonl\", \"w\") as f:\n",
    "    for entry in val_data:\n",
    "        for i in range(len(entry)):\n",
    "            f.write(json.dumps(entry[i]) + \"\\n\")\n",
    "print(\"Grouped data: \",len(grouped_data.keys()))\n",
    "print(\"Length of train data: \", len(train_data))\n",
    "print(\"Length of test data: \", len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[BEGIN DIALOGUE]\n",
      "Student: i finally found out the answer\n",
      "it was 96 \n",
      "Tutor: Yeah! \n",
      "[END DIALOGUE]\n",
      "['confirmatory_feedback']\n",
      "[BEGIN DIALOGUE]\n",
      "Student: i finally found out the answer\n",
      "it was 96 \n",
      "Tutor: Yeah! ['confirmatory_feedback']\n",
      "Student: i guess every single question then watch the video of how you do it \n",
      "Student: what is a thread \n",
      "Teacher: great! a thread is all the posts of the same topic\n",
      "**post closed** \n",
      "[END DIALOGUE]\n",
      "['giving_explanation', 'praising_and_encouraging']\n",
      "[BEGIN DIALOGUE]\n",
      "Student: i finally found out the answer\n",
      "it was 96 \n",
      "Tutor: Yeah! ['confirmatory_feedback']\n",
      "Student: i guess every single question then watch the video of how you do it \n",
      "Student: what is a thread \n",
      "Teacher: great! a thread is all the posts of the same topic\n",
      "**post closed** ['giving_explanation', 'praising_and_encouraging']\n",
      "Student: what does **post closed** mean \n",
      "Tutor: Don't post after her  \n",
      "[END DIALOGUE]\n",
      "['managing_discussions']\n",
      "[BEGIN DIALOGUE]\n",
      "Student: i finally found out the answer\n",
      "it was 96 \n",
      "Tutor: Yeah! ['confirmatory_feedback']\n",
      "Student: i guess every single question then watch the video of how you do it \n",
      "Student: what is a thread \n",
      "Teacher: great! a thread is all the posts of the same topic\n",
      "**post closed** ['giving_explanation', 'praising_and_encouraging']\n",
      "Student: what does **post closed** mean \n",
      "Tutor: Don't post after her  ['managing_discussions']\n",
      "Tutor: sorry, audra, explaining  \n",
      "[END DIALOGUE]\n",
      "['acknowledging_tutor_issue']\n",
      "[BEGIN DIALOGUE]\n",
      "Student: i finally found out the answer\n",
      "it was 96 \n",
      "Tutor: Yeah! ['confirmatory_feedback']\n",
      "Student: i guess every single question then watch the video of how you do it \n",
      "Student: what is a thread \n",
      "Teacher: great! a thread is all the posts of the same topic\n",
      "**post closed** ['giving_explanation', 'praising_and_encouraging']\n",
      "Student: what does **post closed** mean \n",
      "Tutor: Don't post after her  ['managing_discussions']\n",
      "Tutor: sorry, audra, explaining  ['acknowledging_tutor_issue']\n",
      "Teacher: no prob\n",
      "**post now closed** \n",
      "[END DIALOGUE]\n",
      "['managing_discussions']\n",
      "[BEGIN DIALOGUE]\n",
      "Student: y=-/x-2/-2 \n",
      "Teacher: Are those meant to be absolute value bars?  \n",
      "[END DIALOGUE]\n",
      "['asking_for_elaboration']\n",
      "[BEGIN DIALOGUE]\n",
      "Student: y=-/x-2/-2 \n",
      "Teacher: Are those meant to be absolute value bars?  ['asking_for_elaboration']\n",
      "Tutor: Plug in your points and graph them. \n",
      "[END DIALOGUE]\n",
      "['giving_instruction']\n",
      "[BEGIN DIALOGUE]\n",
      "Student: y=-/x-2/-2 \n",
      "Teacher: Are those meant to be absolute value bars?  ['asking_for_elaboration']\n",
      "Tutor: Plug in your points and graph them. ['giving_instruction']\n",
      "Tutor: Yes. They are. \n",
      "[END DIALOGUE]\n",
      "['confirmatory_feedback']\n",
      "[BEGIN DIALOGUE]\n",
      "Student: y=-/x-2/-2 \n",
      "Teacher: Are those meant to be absolute value bars?  ['asking_for_elaboration']\n",
      "Tutor: Plug in your points and graph them. ['giving_instruction']\n",
      "Tutor: Yes. They are. ['confirmatory_feedback']\n",
      "Teacher: Try using the desmos tool to graph :)  \n",
      "[END DIALOGUE]\n",
      "['giving_instruction']\n",
      "[BEGIN DIALOGUE]\n",
      "Student: y=-/x-2/-2 \n",
      "Teacher: Are those meant to be absolute value bars?  ['asking_for_elaboration']\n",
      "Tutor: Plug in your points and graph them. ['giving_instruction']\n",
      "Tutor: Yes. They are. ['confirmatory_feedback']\n",
      "Teacher: Try using the desmos tool to graph :)  ['giving_instruction']\n",
      "Student: That my problem if I plug in lets say -3 and iget -1, -/-1/ what would that be? \n",
      "Student: would it be negative 1 or positive 1? \n",
      "Teacher: It would be negative, because inside the absolute value would be positive and then you add on the negative outside \n",
      "[END DIALOGUE]\n",
      "['giving_explanation', 'giving_answers']\n",
      "Output written to /Users/fareyaikram/Workspace/dialogue-kt-main/fareya/sample_output.jsonl\n"
     ]
    }
   ],
   "source": [
    "DATA_PATH = \"/Users/fareyaikram/Workspace/dialogue-kt-main/fareya/processed_florida_data_1.jsonl\"\n",
    "output_file = \"/Users/fareyaikram/Workspace/dialogue-kt-main/fareya/sample_output.jsonl\"\n",
    "data = read_jsonl(DATA_PATH)\n",
    "grouped_data = group_data_by_id(data)\n",
    "test_data = get_test_formatted(grouped_data)\n",
    "formatted_outputs, labels, ids = format_dialogue_anation(test_data)\n",
    "\n",
    "with open(output_file, 'w') as f:\n",
    "    for formatted_output, label, id_ in zip(formatted_outputs, labels, ids):\n",
    "        json_line = {\n",
    "            \"formatted_output\": formatted_output,\n",
    "            \"label\": label,\n",
    "            \"id\": id_\n",
    "        }\n",
    "        f.write(json.dumps(json_line) + '\\n')\n",
    "\n",
    "print(f\"Output written to {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dk",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
