import torch
from data_loaders import read_jsonl, group_data_by_id, split_train_val, DialogueDatasetUnpacked, DialogueCollatorUnpacked
from transformers import AutoModelForSequenceClassification, AutoTokenizer
from peft import LoraConfig, PeftModel, get_peft_model, prepare_model_for_kbit_training
from torch.utils.data import Dataset, DataLoader


MATHDIAL_DIALOGUE_DESC = "the student is attempting to solve a math problem."
DATA_PATH = '/work/pi_juanzhai_umass_edu/fareya_workspace/dialogue-kt/teacher_moves/processed_data/train.jsonl'
MODEL_NAME = "meta-llama/Llama-3.1-8B-Instruct"
ACCESS_TOKEN = "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"
SYSTEM_PROMPT_TEMPLATE = (
    "You are an experienced math teacher. You are given a dialogue between a student and teacher where {desc} "
    "Your job is to predict the next teacher move. There are four teacher moves: generic, focus, telling, probing. "
    "The dialogue is as follows:"
)


def initialize_model(model_ckpt, num_labels):
    model = AutoModelForSequenceClassification.from_pretrained(
        MODEL_NAME,
        num_labels=4,
        problem_type="single_label_classification",  # Multi-class classification
    )
    return model

def get_checkpoint_path(model_name: str):
    return f"saved_models/{model_name}"

def get_base_model(base_model_name: str, tokenizer: AutoTokenizer, quantize: bool):
    base_model = AutoModelForSequenceClassification.from_pretrained(
        base_model_name,
        pad_token_id=tokenizer.pad_token_id,
        # quantization_config=bnb_config if quantize else None,
        # f32 seems helpful for train/test time consistency when quantizing, bf16 performs best for non-quantized
        torch_dtype=torch.float32 if quantize else torch.bfloat16,
        device_map={"": 0}, 
        num_labels=4, 
        problem_type="single_label_classification",

    )
    base_model.config.use_cache = False
    base_model.config.pretraining_tp = 1
    return base_model

def get_model(base_model_name: str, test: bool,
              model_name: str = None, pt_model_name: str = None,
              r: int = None, lora_alpha: int = None,
              quantize: bool = True, use_gradient_checkpointing: bool = True):
    tokenizer = AutoTokenizer.from_pretrained(base_model_name, padding_side="right")
    tokenizer.pad_token = tokenizer.bos_token # Have to pick some token, and eos triggers a warning
    model = get_base_model(base_model_name, tokenizer, quantize)
    if test and model_name:
        # Note we are loading adapter on quantized model and not merging
        # Recommended here - https://huggingface.co/docs/trl/main/en/dpo_trainer#downsides-to-merging-qlora-before-dpo-approach-2
        # Also prevents empty responses generated by Llama models
        print("Initializing inference-time model from fine-tuned LoRA adapters")
        model = PeftModel.from_pretrained(model, get_checkpoint_path(model_name))
    elif not test:
        if quantize:
            model = prepare_model_for_kbit_training(model, use_gradient_checkpointing=use_gradient_checkpointing)
        if pt_model_name:
            print("Initializing trainable model from pre-trained LoRA adapters")
            model = PeftModel.from_pretrained(model, get_checkpoint_path(pt_model_name), is_trainable=True, adapter_name="default")
        else:
            print("Initializing trainable model with new LoRA adapters")
            peft_config = LoraConfig(
                target_modules=["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"],
                r=r,
                lora_alpha=lora_alpha,
                lora_dropout=0.05,
                task_type="CAUSAL_LM",
                inference_mode=False,
            )
            model = get_peft_model(model, peft_config)
    else:
        print("Initializing inference-time model from pre-trained weights")
    return model, tokenizer

def get_class_tokens(tokenizer, class_labels = ["generic", "focus", "telling", "probing"]):
    """
    Returns the token IDs for all class labels.
    Args:
        tokenizer: The tokenizer for the language model.
        class_labels: A list of class labels (strings).
    Returns:
        class_tokens: A dictionary mapping class labels to their token IDs.
    """
    class_tokens = {
        label: tokenizer(label, return_tensors="pt").input_ids.squeeze()
        for label in class_labels
    }
    return class_tokens

def main():
    # Load data and split into train and val sets
    print("Loading data")
    data = read_jsonl(DATA_PATH)
    grouped_data = group_data_by_id(data)
    train_data, val_data = split_train_val(grouped_data)
    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
    train_dataset = DialogueDatasetUnpacked(train_data, tokenizer)
    collator = DialogueCollatorUnpacked(tokenizer)
    train_loader = DataLoader(train_dataset, batch_size=1, collate_fn=collator)
    val_loader = DataLoader(val_data, batch_size=1, collate_fn=collator)
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(device)
    model = initialize_model(MODEL_NAME, 4).to(device)
        # defaults = {
        #     "epochs": 5,
        #     "lr": 2e-4,
        #     "wd": 1e-2,
        #     "gc": 1.0,
        #     "batch_size": 1,
        #     "grad_accum_steps": 64,
        #     "r": 16,
        #     "lora_alpha": 16
        # }
    train_config = {
        "model_name": MODEL_NAME,
        "pt_model_name": "trainable_lora_model",
        "r": 16,
        "lora_alpha": 16,
        "epochs": 5,
        "lr": 2e-4,
        "wd": 1e-2,
        "gc": 1.0,
        "batch_size": 1,
        "grad_accum_steps": 64,
    }
    # Initialize model
    print("Initializing model")
    model, tokenizer = get_model(MODEL_NAME, 
                                test=False, 
                                model_name=train_config["model_name"], 
                                pt_model_name=train_config["pt_model_name"], 
                                r=train_config["r"], 
                                lora_alpha=train_config["lora_alpha"]) 

main()